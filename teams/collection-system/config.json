{
  "name": "collection-system",
  "description": "Hybrid Image Collection System implementation - DB/Config, Services, Router, Frontend, Tests",
  "createdAt": 1770693342127,
  "leadAgentId": "team-lead@collection-system",
  "leadSessionId": "3f947595-3a1a-4e64-8dbd-61f5a1faa89e",
  "members": [
    {
      "agentId": "team-lead@collection-system",
      "name": "team-lead",
      "agentType": "team-lead",
      "model": "claude-opus-4-6",
      "joinedAt": 1770693342127,
      "tmuxPaneId": "",
      "cwd": "/Users/masaaki_nagasawa/Desktop/prm/aiimg",
      "subscriptions": []
    },
    {
      "agentId": "backend-services@collection-system",
      "name": "backend-services",
      "agentType": "general-purpose",
      "model": "claude-opus-4-6",
      "prompt": "You need to create 3 new service files for the makeimg project at /Users/masaaki_nagasawa/Desktop/prm/aiimg/backend/services/. These are ALL NEW files.\n\n## IMPORTANT PATTERNS TO FOLLOW\n\nThe project uses:\n- `from backend.config import GEMINI_API_KEY` for API keys\n- `from backend.database import get_connection, dict_from_row, dicts_from_rows` for DB access\n- `from google import genai` and `from google.genai import types` for Gemini API\n- `conn = get_connection()` with `try/finally: conn.close()` pattern\n- `async def` for async functions\n- Logging via `logging.getLogger(__name__)`\n- `from backend.config import COLLECTION_DIR, GOOGLE_CSE_API_KEY, GOOGLE_CSE_ID, GEMINI_API_KEY`\n\n---\n\n## File 1: /Users/masaaki_nagasawa/Desktop/prm/aiimg/backend/services/dedup_service.py\n\nCreate this file with perceptual hash deduplication:\n\n```python\n\"\"\"画像重複排除サービス（Perceptual Hash）\"\"\"\nfrom __future__ import annotations\n\nimport logging\nfrom io import BytesIO\nfrom pathlib import Path\nfrom typing import List, Optional, Tuple\n\nimport imagehash\nfrom PIL import Image\n\nfrom backend.database import get_connection, dicts_from_rows\n\nlogger = logging.getLogger(__name__)\n\n\ndef compute_perceptual_hash(image_path: str | Path) -> str:\n    \"\"\"画像ファイルからpHashを計算\"\"\"\n    try:\n        img = Image.open(image_path)\n        phash = imagehash.phash(img)\n        img.close()\n        return str(phash)\n    except Exception as e:\n        logger.error(f\"pHash計算エラー: {image_path} - {e}\")\n        return \"\"\n\n\ndef compute_perceptual_hash_from_bytes(data: bytes) -> str:\n    \"\"\"バイト列からpHashを計算\"\"\"\n    try:\n        img = Image.open(BytesIO(data))\n        phash = imagehash.phash(img)\n        img.close()\n        return str(phash)\n    except Exception as e:\n        logger.error(f\"pHash計算エラー（bytes）: {e}\")\n        return \"\"\n\n\ndef hamming_distance(hash_a: str, hash_b: str) -> int:\n    \"\"\"2つのpHash間のハミング距離を計算\"\"\"\n    if not hash_a or not hash_b:\n        return 64  # 最大距離\n    try:\n        h1 = imagehash.hex_to_hash(hash_a)\n        h2 = imagehash.hex_to_hash(hash_b)\n        return h1 - h2\n    except Exception:\n        return 64\n\n\ndef is_duplicate(phash: str, threshold: int = 8) -> Optional[dict]:\n    \"\"\"既存のreference_imagesとの重複チェック\n    \n    Returns:\n        重複が見つかった場合はその参考画像のdict、なければNone\n    \"\"\"\n    if not phash:\n        return None\n    conn = get_connection()\n    try:\n        rows = conn.execute(\n            \"SELECT id, perceptual_hash, file_path FROM reference_images WHERE perceptual_hash != ''\"\n        ).fetchall()\n        for row in rows:\n            dist = hamming_distance(phash, row[\"perceptual_hash\"])\n            if dist <= threshold:\n                return {\"id\": row[\"id\"], \"file_path\": row[\"file_path\"], \"distance\": dist}\n        return None\n    finally:\n        conn.close()\n\n\ndef find_duplicates_in_job(job_id: int, threshold: int = 8) -> List[Tuple[int, int, int]]:\n    \"\"\"ジョブ内の候補画像間で重複ペアを検出\n    \n    Returns:\n        List of (candidate_id_a, candidate_id_b, hamming_distance)\n    \"\"\"\n    conn = get_connection()\n    try:\n        rows = conn.execute(\n            \"SELECT id, perceptual_hash FROM collection_candidates WHERE job_id = ? AND perceptual_hash != '' AND status != 'duplicate'\",\n            (job_id,)\n        ).fetchall()\n        \n        candidates = [(row[\"id\"], row[\"perceptual_hash\"]) for row in rows]\n        duplicates = []\n        \n        for i in range(len(candidates)):\n            for j in range(i + 1, len(candidates)):\n                dist = hamming_distance(candidates[i][1], candidates[j][1])\n                if dist <= threshold:\n                    duplicates.append((candidates[i][0], candidates[j][0], dist))\n        \n        return duplicates\n    finally:\n        conn.close()\n\n\ndef mark_duplicates(job_id: int, threshold: int = 8) -> int:\n    \"\"\"ジョブ内の重複候補をstatusを'duplicate'に更新。重複排除された数を返す\"\"\"\n    duplicates = find_duplicates_in_job(job_id, threshold)\n    if not duplicates:\n        return 0\n    \n    # 後のIDを重複としてマーク\n    duplicate_ids = set()\n    for _, cid_b, _ in duplicates:\n        duplicate_ids.add(cid_b)\n    \n    conn = get_connection()\n    try:\n        for cid in duplicate_ids:\n            conn.execute(\n                \"UPDATE collection_candidates SET status = 'duplicate' WHERE id = ?\",\n                (cid,)\n            )\n        conn.execute(\n            \"UPDATE collection_jobs SET total_deduplicated = ?, updated_at = datetime('now') WHERE id = ?\",\n            (len(duplicate_ids), job_id)\n        )\n        conn.commit()\n        return len(duplicate_ids)\n    finally:\n        conn.close()\n```\n\n---\n\n## File 2: /Users/masaaki_nagasawa/Desktop/prm/aiimg/backend/services/llm_service.py\n\n```python\n\"\"\"LLMサービス（クエリ拡張・リファインメント）\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport logging\nfrom typing import List\n\nfrom google import genai\nfrom google.genai import types\n\nfrom backend.config import GEMINI_API_KEY\n\nlogger = logging.getLogger(__name__)\n\n\ndef _get_client() -> genai.Client:\n    \"\"\"Gemini APIクライアントを取得\"\"\"\n    return genai.Client(api_key=GEMINI_API_KEY)\n\n\nasync def generate_search_queries(user_query: str, num: int = 5) -> List[str]:\n    \"\"\"ユーザーの検索クエリをLLMで拡張し、複数の検索クエリを生成\n    \n    Args:\n        user_query: 元のクエリ\n        num: 生成するクエリ数\n        \n    Returns:\n        拡張されたクエリのリスト\n    \"\"\"\n    client = _get_client()\n    \n    prompt = f\"\"\"あなたは画像検索の専門家です。以下のクエリに対して、より多様で効果的な画像検索クエリを{num}個生成してください。\n\n元のクエリ: {user_query}\n\n以下の点を考慮してください:\n- 異なる表現やキーワードの組み合わせ\n- 英語と日本語の両方のクエリ\n- 具体的な描写や形容詞の追加\n- 関連するスタイルや雰囲気のキーワード\n\nJSON配列形式で返してください。例: [\"query1\", \"query2\", ...]\n必ずJSON配列のみを返してください。説明文は不要です。\"\"\"\n\n    try:\n        response = client.models.generate_content(\n            model=\"gemini-2.5-flash\",\n            contents=prompt,\n            config=types.GenerateContentConfig(\n                response_mime_type=\"application/json\",\n            ),\n        )\n        \n        if response.text:\n            queries = json.loads(response.text)\n            if isinstance(queries, list):\n                return [str(q) for q in queries[:num]]\n        \n        logger.warning(\"クエリ拡張の応答が不正: %s\", response.text[:200] if response.text else \"empty\")\n        return [user_query]\n    except Exception as e:\n        logger.error(f\"クエリ拡張エラー: {e}\")\n        return [user_query]\n\n\nasync def generate_refinement_queries(original_query: str, feedback: str, num: int = 3) -> List[str]:\n    \"\"\"フィードバックに基づいてクエリをリファインメント\n    \n    Args:\n        original_query: 元のクエリ\n        feedback: ユーザーからのフィードバック\n        num: 生成するクエリ数\n        \n    Returns:\n        リファインされたクエリのリスト\n    \"\"\"\n    client = _get_client()\n    \n    prompt = f\"\"\"以下の画像検索クエリとフィードバックを元に、改善された検索クエリを{num}個生成してください。\n\n元のクエリ: {original_query}\nフィードバック: {feedback}\n\nJSON配列形式で返してください。例: [\"query1\", \"query2\", ...]\n必ずJSON配列のみを返してください。\"\"\"\n\n    try:\n        response = client.models.generate_content(\n            model=\"gemini-2.5-flash\",\n            contents=prompt,\n            config=types.GenerateContentConfig(\n                response_mime_type=\"application/json\",\n            ),\n        )\n        \n        if response.text:\n            queries = json.loads(response.text)\n            if isinstance(queries, list):\n                return [str(q) for q in queries[:num]]\n        \n        return [original_query]\n    except Exception as e:\n        logger.error(f\"クエリリファインメントエラー: {e}\")\n        return [original_query]\n```\n\n---\n\n## File 3: /Users/masaaki_nagasawa/Desktop/prm/aiimg/backend/services/collector.py\n\n```python\n\"\"\"画像収集サービス（Google CSE + Gemini Deep Search フォールバック）\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport logging\nimport uuid\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\n\nimport httpx\nfrom google import genai\nfrom google.genai import types\n\nfrom backend.config import (\n    COLLECTION_DIR,\n    GEMINI_API_KEY,\n    GOOGLE_CSE_API_KEY,\n    GOOGLE_CSE_ID,\n)\nfrom backend.database import get_connection\n\nlogger = logging.getLogger(__name__)\n\n\nasync def search_images(\n    queries: List[str],\n    max_results_per_query: int = 10,\n    use_fallback: bool = True,\n) -> List[Dict]:\n    \"\"\"複数クエリで画像検索を実行\n    \n    Args:\n        queries: 検索クエリリスト\n        max_results_per_query: クエリあたりの最大結果数\n        use_fallback: CSE失敗時にGemini Deep Searchにフォールバック\n        \n    Returns:\n        検索結果のリスト [{source_url, thumbnail_url, page_url, source_name, title, snippet, width, height}]\n    \"\"\"\n    all_results = []\n    seen_urls = set()\n    \n    for query in queries:\n        try:\n            # まずGoogle CSEを試行\n            if GOOGLE_CSE_API_KEY and GOOGLE_CSE_ID:\n                results = await _search_google_cse(query, max_results_per_query)\n            else:\n                results = []\n            \n            # CSEで結果が取れない場合、フォールバック\n            if not results and use_fallback:\n                logger.info(f\"CSE結果なし、Gemini Deep Searchにフォールバック: {query}\")\n                results = await _search_gemini_deep(query, max_results_per_query)\n            \n            # URL重複排除\n            for r in results:\n                if r[\"source_url\"] not in seen_urls:\n                    seen_urls.add(r[\"source_url\"])\n                    all_results.append(r)\n                    \n        except Exception as e:\n            logger.error(f\"検索エラー (query={query}): {e}\")\n            # フォールバック試行\n            if use_fallback:\n                try:\n                    results = await _search_gemini_deep(query, max_results_per_query)\n                    for r in results:\n                        if r[\"source_url\"] not in seen_urls:\n                            seen_urls.add(r[\"source_url\"])\n                            all_results.append(r)\n                except Exception as e2:\n                    logger.error(f\"フォールバックも失敗: {e2}\")\n    \n    return all_results\n\n\nasync def _search_google_cse(query: str, max_results: int = 10) -> List[Dict]:\n    \"\"\"Google Custom Search Engine APIで画像検索\"\"\"\n    results = []\n    \n    async with httpx.AsyncClient(timeout=30) as client:\n        params = {\n            \"key\": GOOGLE_CSE_API_KEY,\n            \"cx\": GOOGLE_CSE_ID,\n            \"q\": query,\n            \"searchType\": \"image\",\n            \"num\": min(max_results, 10),  # CSE max is 10 per request\n        }\n        \n        response = await client.get(\n            \"https://www.googleapis.com/customsearch/v1\",\n            params=params,\n        )\n        \n        if response.status_code == 429:\n            logger.warning(\"Google CSE API枯渇（429 Too Many Requests）\")\n            return []\n        \n        response.raise_for_status()\n        data = response.json()\n        \n        for item in data.get(\"items\", []):\n            image_info = item.get(\"image\", {})\n            results.append({\n                \"source_url\": item.get(\"link\", \"\"),\n                \"thumbnail_url\": image_info.get(\"thumbnailLink\", \"\"),\n                \"page_url\": image_info.get(\"contextLink\", \"\"),\n                \"source_name\": item.get(\"displayLink\", \"\"),\n                \"title\": item.get(\"title\", \"\"),\n                \"snippet\": item.get(\"snippet\", \"\"),\n                \"width\": image_info.get(\"width\", 0),\n                \"height\": image_info.get(\"height\", 0),\n            })\n    \n    return results\n\n\nasync def _search_gemini_deep(query: str, max_results: int = 10) -> List[Dict]:\n    \"\"\"Gemini Deep Search（grounding with Google Search）で画像URL取得\"\"\"\n    client = genai.Client(api_key=GEMINI_API_KEY)\n    \n    prompt = f\"\"\"Search for high-quality images related to: {query}\n\nReturn a JSON array of image results. Each result should have:\n- source_url: direct URL to the image file\n- page_url: URL of the page containing the image\n- title: title or description\n- snippet: brief description\n- source_name: website name\n\nReturn ONLY a JSON array, no other text. Example:\n[{{\"source_url\": \"https://example.com/image.jpg\", \"page_url\": \"https://example.com/page\", \"title\": \"...\", \"snippet\": \"...\", \"source_name\": \"example.com\"}}]\n\nFind up to {max_results} images.\"\"\"\n\n    try:\n        response = client.models.generate_content(\n            model=\"gemini-2.5-flash\",\n            contents=prompt,\n            config=types.GenerateContentConfig(\n                tools=[types.Tool(google_search=types.GoogleSearch())],\n            ),\n        )\n        \n        results = []\n        \n        # テキストからJSON抽出を試行\n        if response.text:\n            text = response.text.strip()\n            # JSON配列を探す\n            start = text.find(\"[\")\n            end = text.rfind(\"]\") + 1\n            if start >= 0 and end > start:\n                try:\n                    items = json.loads(text[start:end])\n                    for item in items[:max_results]:\n                        results.append({\n                            \"source_url\": item.get(\"source_url\", \"\"),\n                            \"thumbnail_url\": \"\",\n                            \"page_url\": item.get(\"page_url\", \"\"),\n                            \"source_name\": item.get(\"source_name\", \"\"),\n                            \"title\": item.get(\"title\", \"\"),\n                            \"snippet\": item.get(\"snippet\", \"\"),\n                            \"width\": 0,\n                            \"height\": 0,\n                        })\n                except json.JSONDecodeError:\n                    logger.warning(\"Gemini Deep Search JSON解析失敗\")\n        \n        # grounding_metadataからも補完\n        if hasattr(response, 'candidates') and response.candidates:\n            candidate = response.candidates[0]\n            grounding = getattr(candidate, 'grounding_metadata', None)\n            if grounding and hasattr(grounding, 'grounding_chunks'):\n                for chunk in grounding.grounding_chunks:\n                    web = getattr(chunk, 'web', None)\n                    if web:\n                        uri = getattr(web, 'uri', '')\n                        title = getattr(web, 'title', '')\n                        if uri and uri not in [r[\"page_url\"] for r in results]:\n                            results.append({\n                                \"source_url\": uri,\n                                \"thumbnail_url\": \"\",\n                                \"page_url\": uri,\n                                \"source_name\": \"\",\n                                \"title\": title,\n                                \"snippet\": \"\",\n                                \"width\": 0,\n                                \"height\": 0,\n                            })\n        \n        return results[:max_results]\n    except Exception as e:\n        logger.error(f\"Gemini Deep Search エラー: {e}\")\n        return []\n\n\nasync def download_candidate(source_url: str, timeout: int = 30) -> Optional[Dict]:\n    \"\"\"画像をダウンロード\n    \n    Returns:\n        成功時: {\"file_path\": str, \"file_size\": int, \"content_type\": str}\n        失敗時: None\n    \"\"\"\n    try:\n        async with httpx.AsyncClient(\n            timeout=timeout,\n            follow_redirects=True,\n            headers={\"User-Agent\": \"Mozilla/5.0 (compatible; makeimg-collector/1.0)\"},\n        ) as client:\n            response = await client.get(source_url)\n            response.raise_for_status()\n            \n            content_type = response.headers.get(\"content-type\", \"\")\n            if not content_type.startswith(\"image/\"):\n                logger.warning(f\"非画像コンテンツ: {content_type} ({source_url})\")\n                return None\n            \n            # 拡張子を決定\n            ext_map = {\n                \"image/jpeg\": \".jpg\",\n                \"image/png\": \".png\",\n                \"image/webp\": \".webp\",\n                \"image/gif\": \".gif\",\n            }\n            ext = ext_map.get(content_type.split(\";\")[0].strip(), \".jpg\")\n            \n            # ファイル保存\n            filename = f\"col_{uuid.uuid4().hex[:12]}{ext}\"\n            file_path = COLLECTION_DIR / filename\n            file_path.write_bytes(response.content)\n            \n            return {\n                \"file_path\": str(file_path),\n                \"file_size\": len(response.content),\n                \"content_type\": content_type,\n                \"data\": response.content,\n            }\n    except Exception as e:\n        logger.error(f\"ダウンロードエラー: {source_url} - {e}\")\n        return None\n\n\nasync def download_candidates_batch(candidate_ids: List[int], job_id: int) -> Dict:\n    \"\"\"選択された候補を一括ダウンロード（BackgroundTasks用）\n    \n    Returns:\n        {\"downloaded\": int, \"failed\": int, \"errors\": List[str]}\n    \"\"\"\n    from backend.services.dedup_service import compute_perceptual_hash\n    from backend.services.image_storage import save_reference_image\n    \n    downloaded = 0\n    failed = 0\n    errors = []\n    \n    conn = get_connection()\n    try:\n        for cid in candidate_ids:\n            row = conn.execute(\n                \"SELECT * FROM collection_candidates WHERE id = ? AND job_id = ?\",\n                (cid, job_id)\n            ).fetchone()\n            \n            if not row:\n                continue\n            \n            source_url = row[\"source_url\"]\n            \n            try:\n                result = await download_candidate(source_url)\n                if result:\n                    # 参考画像として保存\n                    file_info = save_reference_image(result[\"data\"], f\"collection{uuid.uuid4().hex[:4]}.jpg\")\n                    \n                    # pHash計算\n                    from backend.config import PROJECT_ROOT\n                    abs_path = PROJECT_ROOT / file_info[\"file_path\"]\n                    phash = compute_perceptual_hash(abs_path) if abs_path.exists() else \"\"\n                    \n                    # reference_imagesに登録\n                    cursor = conn.execute(\"\"\"\n                        INSERT INTO reference_images \n                        (file_path, thumbnail_path, source_url, source_name, category, tags, \n                         width, height, file_size, collection_job_id, perceptual_hash)\n                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n                    \"\"\", (\n                        file_info[\"file_path\"],\n                        file_info[\"thumbnail_path\"],\n                        source_url,\n                        row[\"source_name\"],\n                        \"collection\",\n                        \"\",\n                        file_info[\"width\"],\n                        file_info[\"height\"],\n                        file_info[\"file_size\"],\n                        job_id,\n                        phash,\n                    ))\n                    \n                    # candidateを更新\n                    conn.execute(\"\"\"\n                        UPDATE collection_candidates \n                        SET status = 'downloaded', reference_id = ?, perceptual_hash = ?\n                        WHERE id = ?\n                    \"\"\", (cursor.lastrowid, phash, cid))\n                    \n                    conn.commit()\n                    downloaded += 1\n                else:\n                    conn.execute(\n                        \"UPDATE collection_candidates SET status = 'failed' WHERE id = ?\",\n                        (cid,)\n                    )\n                    conn.commit()\n                    failed += 1\n                    errors.append(f\"Download failed: {source_url}\")\n                    \n            except Exception as e:\n                conn.execute(\n                    \"UPDATE collection_candidates SET status = 'failed' WHERE id = ?\",\n                    (cid,)\n                )\n                conn.commit()\n                failed += 1\n                errors.append(f\"{source_url}: {str(e)}\")\n        \n        # ジョブ更新\n        conn.execute(\"\"\"\n            UPDATE collection_jobs \n            SET total_downloaded = ?, stage = 'downloaded', updated_at = datetime('now')\n            WHERE id = ?\n        \"\"\", (downloaded, job_id))\n        conn.commit()\n        \n    finally:\n        conn.close()\n    \n    return {\"downloaded\": downloaded, \"failed\": failed, \"errors\": errors}\n```\n\nCreate all 3 files using the Write tool. Use EXACTLY the code provided above - do not modify it.",
      "color": "green",
      "planModeRequired": false,
      "joinedAt": 1770693595920,
      "tmuxPaneId": "in-process",
      "cwd": "/Users/masaaki_nagasawa/Desktop/prm/aiimg",
      "subscriptions": [],
      "backendType": "in-process"
    }
  ]
}